{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1a0kaYpkA3k8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete the functions and the other sections below to build a complete model of a simple neural network (equivalent to Logistic Regression). \n",
        "The incomplete parts are marked and you need to fill them up to finish this assignment\n",
        "\n",
        "DATA: handwritten digits classified into two classes Even (0) and Odd (1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0asaSeRoA3lA",
        "outputId": "4759f878-be00-4afa-e180-f564ead7291d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1597, 8, 8) (1, 1597) (200, 8, 8) (1, 200)\n",
            "y = [1], it's a 'odd' number.\n",
            "Number of training examples: m_train = 1597\n",
            "Number of testing examples: m_test = 200\n",
            "Height/Width of each image: num_px = 8\n",
            "Each image is of size: (8, 8)\n",
            "train_set_x shape: (1597, 8, 8)\n",
            "train_set_y shape: (1, 1597)\n",
            "test_set_x shape: (200, 8, 8)\n",
            "test_set_y shape: (1, 200)\n",
            "train_set_x_flatten shape: (64, 1597)\n",
            "train_set_y shape: (1, 1597)\n",
            "test_set_x_flatten shape: (64, 200)\n",
            "test_set_y shape: (1, 200)\n",
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 100: 0.690107\n",
            "Cost after iteration 200: 0.687121\n",
            "Cost after iteration 300: 0.684170\n",
            "Cost after iteration 400: 0.681253\n",
            "Cost after iteration 500: 0.678368\n",
            "Cost after iteration 600: 0.675516\n",
            "Cost after iteration 700: 0.672697\n",
            "Cost after iteration 800: 0.669909\n",
            "Cost after iteration 900: 0.667152\n",
            "Cost after iteration 1000: 0.664427\n",
            "Cost after iteration 1100: 0.661733\n",
            "Cost after iteration 1200: 0.659068\n",
            "Cost after iteration 1300: 0.656434\n",
            "Cost after iteration 1400: 0.653829\n",
            "Cost after iteration 1500: 0.651253\n",
            "Cost after iteration 1600: 0.648705\n",
            "Cost after iteration 1700: 0.646186\n",
            "Cost after iteration 1800: 0.643696\n",
            "Cost after iteration 1900: 0.641232\n",
            "Cost after iteration 2000: 0.638796\n",
            "Cost after iteration 2100: 0.636387\n",
            "Cost after iteration 2200: 0.634004\n",
            "Cost after iteration 2300: 0.631647\n",
            "Cost after iteration 2400: 0.629316\n",
            "Cost after iteration 2500: 0.627011\n",
            "Cost after iteration 2600: 0.624731\n",
            "Cost after iteration 2700: 0.622475\n",
            "Cost after iteration 2800: 0.620244\n",
            "Cost after iteration 2900: 0.618037\n",
            "Cost after iteration 3000: 0.615854\n",
            "Cost after iteration 3100: 0.613695\n",
            "Cost after iteration 3200: 0.611558\n",
            "Cost after iteration 3300: 0.609445\n",
            "Cost after iteration 3400: 0.607354\n",
            "Cost after iteration 3500: 0.605285\n",
            "Cost after iteration 3600: 0.603238\n",
            "Cost after iteration 3700: 0.601213\n",
            "Cost after iteration 3800: 0.599209\n",
            "Cost after iteration 3900: 0.597226\n",
            "train accuracy: 83.40638697557921 %\n",
            "test accuracy: 85.0 %\n",
            "y = 0, you predicted that it is a \"even\" number.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete the functions and the other sections below to build a complete model of a simple neural network (equivalent to Logistic Regression). \n",
        "The incomplete parts are marked and you need to fill them up to finish this assignment\n",
        "\n",
        "DATA: handwritten digits classified into two classes Even (0) and Odd (1)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "##Loading the data\n",
        "digits = datasets.load_digits()\n",
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "\n",
        "train_set_x_orig = [] \n",
        "train_set_y = []\n",
        "test_set_x_orig = [] \n",
        "test_set_y = []\n",
        "classes = ['even','odd']\n",
        "\n",
        "\n",
        "data_size = len(images_and_labels)\n",
        "#Setting the testset size\n",
        "test_set_size = 200\n",
        "\n",
        "\n",
        "##Splitting the data into training and test sets and assigning the labels: Even (0), Odd (1)\n",
        "\n",
        "for i in range(len(images_and_labels))[:-test_set_size]:\n",
        "    train_set_x_orig.append(images_and_labels[i][0])\n",
        "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
        "        train_set_y.append(0)\n",
        "    else: ##if odd put 1 as the label\n",
        "        train_set_y.append(1)\n",
        "        \n",
        "for i in range(len(images_and_labels))[-test_set_size:]:\n",
        "    test_set_x_orig.append(images_and_labels[i][0])\n",
        "    if images_and_labels[i][1] %2 == 0: ##if even put 0 as the label\n",
        "        test_set_y.append(0)\n",
        "    else: ##if odd put 1 as the label\n",
        "        test_set_y.append(1)\n",
        "\n",
        "train_set_x_orig = np.array(train_set_x_orig)\n",
        "train_set_y = np.array(train_set_y).reshape(1,data_size-test_set_size)\n",
        "test_set_x_orig = np.array(test_set_x_orig)\n",
        "test_set_y = np.array(test_set_y).reshape(1,test_set_size)\n",
        "\n",
        "print(train_set_x_orig.shape, train_set_y.shape, test_set_x_orig.shape, test_set_y.shape)\n",
        "\n",
        "## Display an example from the data\n",
        "index = 1\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])] +  \"' number.\")\n",
        "\n",
        "m_train = train_set_y.shape[1]\n",
        "m_test = test_set_y.shape[1]\n",
        "num_px = train_set_x_orig.shape[2]\n",
        "\n",
        "##Description of the data\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \")\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# Reshape the training and test examples from 2D matrix to a 1D vector. The input vector of a neural network is always 1D\n",
        "train_set_x_flatten = train_set_x_orig.reshape(m_train,(num_px*num_px)).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(m_test,(num_px*num_px)).T\n",
        "\n",
        "# Print the description after reshaping\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "\n",
        "# Standardize the pixel value\n",
        "train_set_x = train_set_x_flatten/255.\n",
        "test_set_x = test_set_x_flatten/255.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "    ## Put your code here\n",
        "\n",
        "    s = 1/(1+np.exp(-z))\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ## Put your code here\n",
        "    #initializing a zero matrix for weights\n",
        "    w = np.zeros((dim,1))\n",
        "    #random initialization using numpy random function\n",
        "    # w = np.random.rand(dim,1)\n",
        "    b = 0\n",
        "    \n",
        "    ## veryifying the shape of the w vector\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if even and 1 if odd) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = float(X.shape[1])\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = 1 / (1 + np.exp(-(np.dot(w.T, X) + b))) # put the activation computation code here\n",
        "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) # put the cost computation code here\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRADIENTS)\n",
        "    \n",
        "    dw = (1 / m) * np.dot(X, (A - Y).T) # put the code to compute dw\n",
        "    db =  (1 / m) * np.sum(A - Y) # put the code to compute db\n",
        "    \n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if even, 1 if odd), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "                 \n",
        "        grads, cost = propagate(w, b, X, Y) # call the function that does this part\n",
        "        \n",
        "        \n",
        "        \n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        w = w - learning_rate * dw # update w using dw and learning rate\n",
        "        b = b - learning_rate * db # update b using db and learning rate\n",
        "        \n",
        "        # Recordint the cost after every 100 iterations\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px, 1) in this case (8 * 8, 1)\n",
        "    b -- bias, a scalar \n",
        "    X -- data of size (num_px * num_px, number of examples) in this case (8 * 8, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    ## put the code to predict from input data [prediction refers to computing the 'a' variable or sigmoid of w.T*x+b, \n",
        "    ## where w,b is the learned parameters and x is the input vector of the image you are trying to predict]\n",
        "\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i], i.e. if probability >=0.5 output 1, else 0\n",
        "        Y_prediction[0,i] = 1 if A[0, i] >= 0.5 else 0 ##complete this line\n",
        "        \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction\n",
        "\n",
        "\n",
        "\n",
        "###COMPLETE THE FUNCTION\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### COMPLETE THE CODE BELOW ###\n",
        "    \n",
        "    # initialize parameters with zeros \n",
        "    w, b = initialize_with_zeros(X_train.shape[0]) # call the correct function from above\n",
        "\n",
        "    # Gradient descent \n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # call the correct function from above\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\" returned by the function above\n",
        "    w = parameters[\"w\"] # complete this line\n",
        "    b = parameters[\"b\"] # complete this line\n",
        "    \n",
        "    # Predict test/train set examples \n",
        "    Y_prediction_test = predict(w, b, X_test) # call the correct function from above\n",
        "    Y_prediction_train = predict(w, b, X_train) # call the correct function from above\n",
        "\n",
        "    \n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Calling the function to train the model. FEEL FREE TO MODIFY THE HYPERPARAMETERS (num_iterations AND learning_rate) to experiment with the accuracy\n",
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 4000, learning_rate = 0.1, print_cost = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Using the model you built to predict the digit of an image\n",
        "index = 76 ## put a number between 0 and test_set_size to see the prediction of that image\n",
        "plt.imshow(test_set_x[:,index].reshape((num_px, num_px)))\n",
        "# print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\") +  \"\\\" number.\")\n",
        "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])] +  \"\\\" number.\")\n",
        "\n",
        "\n",
        "\n",
        "## View how the cost varied with iterations\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
