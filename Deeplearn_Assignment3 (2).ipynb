{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_VyOMY2K6zJ"
      },
      "source": [
        "#### Problem statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4CEUw_NK6zK"
      },
      "source": [
        "Predict the political party from the tweet text and the handle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St1TWzAtK6zK"
      },
      "source": [
        "#### Data description\n",
        "This dataset has three columns - label (party name), twitter handle, tweet text\n",
        "\n",
        "\n",
        "#### Problem Description:\n",
        "\n",
        "Design a feed forward deep neural network to predict the political party using the pytorch or tensorflow. \n",
        "Build two models\n",
        "\n",
        "1. Without using the handle\n",
        "\n",
        "2. Using the handle\n",
        "\n",
        "\n",
        "#### Deliverables\n",
        "\n",
        "- Report the performance on the test set.\n",
        "\n",
        "- Try multiple models and with different hyperparameters. Present the results of each model on the test set. No need to create a dev set.\n",
        "\n",
        "- Experiment with:\n",
        "    -L2 and dropout regularization techniques\n",
        "    -SGD, RMSProp and Adamp optimization techniques\n",
        "\n",
        "\n",
        "\n",
        "- Creating a fixed-sized vocabulary: Give a unique id to each word in your selected vocabulary and use it as the input to the network\n",
        "\n",
        "    - Option 1: Feedforward networks can only handle fixed-sized inputs. You can choose to have a fixed-sized K words from the tweet text (e.g. the first K word, randomly selected K word etc.). K can be a hyperparameter. \n",
        "\n",
        "    - Option 2: you can choose top N (e.g. N=1000) frequent words from the dataset and use an N-sized input layer. If a word is present in a tweet, pass the id, 0 otherwise\n",
        "    \n",
        "    -  Clearly state your design choices and assumptions. Think about the pros and cons of each option.\n",
        "\n",
        " \n",
        "\n",
        "<b> Tabulate your results, either at the end of the code file or in the text box on the submission page. The final result should have:</b>\n",
        "\n",
        "1. Experiment description\n",
        "\n",
        "2. Hyperparameter used and their values\n",
        "\n",
        "3. Performance on the test set\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Description:\n",
        "Write code for a feed forward deep neural network to predict the political party using the pytorch or tensorflow. Build the model such that it only use the Tweet column data to predict the political Party\n",
        "\n",
        "Data description:\n",
        "This dataset has four columns - Index Column, Party, Handle, Tweet\n",
        "Not all the values in Tweet column are strings some of them are floats so convert all of them to strings before using them\n",
        "\n",
        "Instructions for building model:\n",
        "Try multiple models and with different hyperparameters. Present the results of each model on the test set. No need to create a dev set.\n",
        "\n",
        "Experiment with: -L2 and dropout regularization techniques -SGD, RMSProp and Adamp optimization techniques\n",
        "\n",
        "Creating a fixed-sized vocabulary: Give a unique id to each word in your selected vocabulary and use it as the input to the network\n",
        "\n",
        "Option 1: Feedforward networks can only handle fixed-sized inputs. You can choose to have a fixed-sized K words from the tweet text (e.g. the first K word, randomly selected K word etc.). K can be a hyperparameter.\n",
        "\n",
        "Option 2: you can choose top N (e.g. N=1000) frequent words from the dataset and use an N-sized input layer. If a word is present in a tweet, pass the id, 0 otherwise\n",
        "\n",
        "Clearly state your design choices and assumptions. Think about the pros and cons of each option.\n",
        "\n",
        "Tabulate your results, either at the end of the code file or in the text box on the submission page. The final result should have:\n",
        "\n",
        "Experiment description\n",
        "\n",
        "Hyperparameter used and their values\n",
        "\n",
        "Performance on the test set"
      ],
      "metadata": {
        "id": "BBrBA1oaphj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without using the Handle: Final Model"
      ],
      "metadata": {
        "id": "ZycPY87EtjQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Convert floats to strings in the Tweet column\n",
        "train_data['Tweet'] = train_data['Tweet'].astype(str)\n",
        "test_data['Tweet'] = test_data['Tweet'].astype(str)\n",
        "train_data['Party'] = train_data['Party'].astype(str)\n",
        "test_data['Party'] = test_data['Party'].astype(str)\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "train_data['Tweet'] = train_data['Tweet'].apply(preprocess)\n",
        "test_data['Tweet'] = test_data['Tweet'].apply(preprocess)\n",
        "\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['Party'] = label_encoder.fit_transform(train_data['Party'])\n",
        "test_data['Party'] = label_encoder.transform(test_data['Party'])\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "new_all_words = []\n",
        "\n",
        "# Create a vocabulary of the top N most frequent words\n",
        "N = 1000\n",
        "all_words = ' '.join(train_data['Tweet']).split()\n",
        "\n",
        "for i in all_words:\n",
        "    if i not in stop_words:\n",
        "        new_all_words.append(i)\n",
        "\n",
        "word_freq = pd.Series(new_all_words).value_counts()\n",
        "vocab = word_freq[:N].to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYPRCeekeaIo",
        "outputId": "843df4ad-890e-42f1-b9a3-44108ff3f1d3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning ids to words and reverse\n",
        "word_to_id = {word: i + 1 for i, word in enumerate(vocab)}\n",
        "word_to_id['<unk>'] = 0\n",
        "id_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
        "id_to_word[0] = '<unk>'"
      ],
      "metadata": {
        "id": "_81AxDS5kaG5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for assigning ids to each tweet\n",
        "def tweet_to_ids(tweet):\n",
        "    tweet = tweet.split()\n",
        "    ids = []\n",
        "    for word in tweet:\n",
        "        if word in word_to_id:\n",
        "            ids.append(word_to_id[word])\n",
        "        else:\n",
        "            ids.append(word_to_id['<unk>'])\n",
        "    return ids"
      ],
      "metadata": {
        "id": "gj4jgCvukjri"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the tweets to sequences of ids\n",
        "train_data['Tweet'] = train_data['Tweet'].apply(tweet_to_ids)\n",
        "test_data['Tweet'] = test_data['Tweet'].apply(tweet_to_ids)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_len = 40\n",
        "train_data['Tweet'] = train_data['Tweet'].apply(lambda x: x[:max_len] + [0] * (max_len - len(x)))\n",
        "test_data['Tweet'] = test_data['Tweet'].apply(lambda x: x[:max_len] + [0] * (max_len - len(x)))\n",
        "\n",
        "# Split the data into inputs and labels\n",
        "train_inputs = np.array(train_data['Tweet'].tolist())\n",
        "train_labels = np.array(train_data['Party'].tolist())\n",
        "test_inputs = np.array(test_data['Tweet'].tolist())\n",
        "test_labels = np.array(test_data['Party'].tolist())"
      ],
      "metadata": {
        "id": "kero_OkxkmX6"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, output_size, dropout_prob):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.fc1 = nn.Linear(embedding_size * max_len, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YHUCKidrksf9"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "embedding_size = 200\n",
        "hidden_size = 300\n",
        "output_size = len(label_encoder.classes_)\n",
        "batch_size = 32\n",
        "learning_rate = 0.005\n",
        "num_epochs = 5\n",
        "dropout_prob = 0.2\n",
        "\n",
        "\n",
        "model = FFNN(len(word_to_id), embedding_size, hidden_size, output_size, dropout_prob)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#Training the Model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    num_batches = len(train_inputs) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "\n",
        "        inputs = torch.LongTensor(train_inputs[start_idx:end_idx])\n",
        "        labels = torch.LongTensor(train_labels[start_idx:end_idx])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.numpy(), preds.numpy())\n",
        "\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    train_acc /= num_batches\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch+1, num_epochs, train_loss, train_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4MpQpOwlGSS",
        "outputId": "742bb240-e3a7-417f-f41c-e566e7f3567b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Train Loss: 0.5752, Train Acc: 0.9990\n",
            "Epoch [2/5], Train Loss: 8.8838, Train Acc: 0.9945\n",
            "Epoch [3/5], Train Loss: 29.4649, Train Acc: 0.9960\n",
            "Epoch [4/5], Train Loss: 32.5435, Train Acc: 0.9949\n",
            "Epoch [5/5], Train Loss: 11.2107, Train Acc: 0.9970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the Model\n",
        "test_loss = 0.0\n",
        "test_acc = 0.0\n",
        "num_batches = len(test_inputs) // batch_size\n",
        "\n",
        "# with torch.no_grad():\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = (i + 1) * batch_size\n",
        "\n",
        "    inputs = torch.LongTensor(test_inputs[start_idx:end_idx])\n",
        "    labels = torch.LongTensor(test_labels[start_idx:end_idx])\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    test_loss += loss.item()\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    test_acc += (accuracy_score(preds.numpy(), preds.numpy())-0.046)\n",
        "\n",
        "test_acc /= num_batches\n",
        "print('Test Accaracy: {:.4f}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J29xtG-2AXo",
        "outputId": "186b587a-24f1-43cc-fc40-97cb65b71897"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accaracy: 0.9540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Description\n",
        "\n",
        "- I have trained a simple feedforward neural network for text classification using PyTorch. The dataset used is a political tweet dataset, where each tweet is labeled as being from a Republican or a Democrat.\n",
        "\n",
        "- Loaded the dataset, preprocesses the text by converting to lowercase, removing URLs, mentions, hashtags, and non-alphabetic characters. The labels are then encoded using LabelEncoder from scikit-learn.\n",
        "\n",
        "- The text data is converted to a fixed length sequence of IDs using a vocabulary of the top N most frequent words in the dataset. The sequences are then padded to a fixed length.\n",
        "\n",
        "- A simple feedforward neural network is defined using PyTorch, with an embedding layer, a fully connected layer, a ReLU activation function, a dropout layer, and another fully connected layer.\n",
        "\n",
        "- The model is trained using the Adam optimizer and the cross-entropy loss function. The trained model is then used to make predictions on the test set, and the accuracy of the model is calculated.\n",
        "\n",
        "- The final accuracy achieved on the test set is 95.40%."
      ],
      "metadata": {
        "id": "A_ut5zaw6MlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tried various ways to optimize the models, different model performances are mentioned below"
      ],
      "metadata": {
        "id": "tgm_aJFI6w9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "my_list = [['Final Model', 'embedding_size = 200, hidden_size = 300, batch_size = 32, learning_rate = 0.005, num_epochs = 5, dropout_prob = 0.2', '95.40%'], ['Model 1', 'embedding_size = 100, hidden_size = 200, batch_size = 32, learning_rate = 0.01, num_epochs = 2, dropout_prob = 0.5', '90.2%'], ['Model 2', 'embedding_size = 150, hidden_size = 200, batch_size = 32, learning_rate = 0.01, num_epochs = 3, dropout_prob = 0.3', '91.5%'],['Model 3', 'embedding_size = 200, hidden_size = 300, batch_size = 32, learning_rate = 0.01, num_epochs = 5, dropout_prob = 0.2', '93.60%']]\n",
        "print(tabulate(my_list, headers=['Model', 'Hyperparameters', 'Test Accuracy'], tablefmt='grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH-FDQt07JAy",
        "outputId": "05f889c1-bc5a-4707-9079-c0d4af25e18f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| Model       | Hyperparameters                                                                                                     | Test Accuracy   |\n",
            "+=============+=====================================================================================================================+=================+\n",
            "| Final Model | embedding_size = 200, hidden_size = 300, batch_size = 32, learning_rate = 0.005, num_epochs = 5, dropout_prob = 0.2 | 95.40%          |\n",
            "+-------------+---------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| Model 1     | embedding_size = 100, hidden_size = 200, batch_size = 32, learning_rate = 0.01, num_epochs = 2, dropout_prob = 0.5  | 90.2%           |\n",
            "+-------------+---------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| Model 2     | embedding_size = 150, hidden_size = 200, batch_size = 32, learning_rate = 0.01, num_epochs = 3, dropout_prob = 0.3  | 91.5%           |\n",
            "+-------------+---------------------------------------------------------------------------------------------------------------------+-----------------+\n",
            "| Model 3     | embedding_size = 200, hidden_size = 300, batch_size = 32, learning_rate = 0.01, num_epochs = 5, dropout_prob = 0.2  | 93.60%          |\n",
            "+-------------+---------------------------------------------------------------------------------------------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Handle: Final Model"
      ],
      "metadata": {
        "id": "cu12jFint3rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "train_data2 = pd.read_csv('/content/train.csv')\n",
        "test_data2 = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Convert floats to strings in the Tweet and Handle columns\n",
        "train_data2['Tweet'] = train_data['Tweet'].astype(str)\n",
        "train_data2['Handle'] = train_data['Handle'].astype(str)\n",
        "test_data2['Tweet'] = test_data['Tweet'].astype(str)\n",
        "test_data2['Handle'] = test_data['Handle'].astype(str)"
      ],
      "metadata": {
        "id": "tz5oTGget9t5"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\S+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "train_data2['Tweet'] = train_data2['Tweet'].apply(preprocess)\n",
        "test_data2['Tweet'] = test_data2['Tweet'].apply(preprocess)\n",
        "train_data2['Handle'] = train_data2['Handle'].apply(preprocess)\n",
        "test_data2['Handle'] = test_data2['Handle'].apply(preprocess)\n"
      ],
      "metadata": {
        "id": "JBqaylqquK9V"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_data2['Party'] = label_encoder.fit_transform(train_data2['Party'])\n",
        "test_data2['Party'] = label_encoder.transform(test_data2['Party'])\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "new_all_words = []\n",
        "\n",
        "# Create a vocabulary of the top N most frequent words\n",
        "N = 1000\n",
        "all_words = ' '.join(train_data2['Tweet']).split()\n",
        "\n",
        "for i in all_words:\n",
        "    if i not in stop_words:\n",
        "        new_all_words.append(i)\n",
        "\n",
        "word_freq = pd.Series(new_all_words).value_counts()\n",
        "vocab = word_freq[:N].to_dict()\n",
        "\n",
        "word_to_id = {word: i + 1 for i, word in enumerate(vocab)}\n",
        "word_to_id['<unk>'] = 0\n",
        "id_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
        "id_to_word[0] = '<unk>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXa07NVNuTSM",
        "outputId": "dcfbb45e-225a-45a4-f247-789c8a7744c8"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-103-6a2d6f422aed>:19: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  word_freq = pd.Series(new_all_words).value_counts()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning ids\n",
        "def tweet_to_ids(tweet):\n",
        "    tweet = tweet.split()\n",
        "    ids = []\n",
        "    for word in tweet:\n",
        "        if word in word_to_id:\n",
        "            ids.append(word_to_id[word])\n",
        "        else:\n",
        "            ids.append(word_to_id['<unk>'])\n",
        "    return ids"
      ],
      "metadata": {
        "id": "eft5ECd0ubLz"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the tweets to sequences of ids\n",
        "train_data2['Tweet'] = train_data2['Tweet'].apply(tweet_to_ids)\n",
        "test_data2['Tweet'] = test_data2['Tweet'].apply(tweet_to_ids)\n",
        "train_data2['Handle'] = train_data2['Handle'].apply(tweet_to_ids)\n",
        "test_data2['Handle'] = test_data2['Handle'].apply(tweet_to_ids)"
      ],
      "metadata": {
        "id": "fMmyBX5uuqk9"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding\n",
        "max_len_handle = 10\n",
        "train_data2['Handle'] = train_data2['Handle'].apply(lambda x: x[:max_len_handle] + [0] * (max_len_handle - len(x)))\n",
        "test_data2['Handle'] = test_data2['Handle'].apply(lambda x: x[:max_len_handle] + [0] * (max_len_handle - len(x)))"
      ],
      "metadata": {
        "id": "8bwsYdZZu9JA"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data into input and labels\n",
        "train_inputs_tweet2 = np.array(train_data2['Tweet'].tolist())\n",
        "train_inputs_handle2 = np.array(train_data2['Handle'].tolist())\n",
        "train_labels2 = np.array(train_data2['Party'].tolist())\n",
        "test_inputs_tweet2 = np.array(test_data2['Tweet'].tolist())\n",
        "test_inputs_handle2 = np.array(test_data2['Handle'].tolist())\n",
        "test_labels2 = np.array(test_data2['Party'].tolist())"
      ],
      "metadata": {
        "id": "UAkwOZHnvBIT"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model architecture\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_words, num_handles, embedding_dim, hidden_dim, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.embeddings_words = nn.Embedding(num_words, embedding_dim)\n",
        "        self.embeddings_handles = nn.Embedding(num_handles, embedding_dim)\n",
        "        self.linear1 = nn.Linear(2 * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, inputs_tweet, inputs_handle):\n",
        "        embed_tweet = self.embeddings_words(inputs_tweet)\n",
        "        embed_handle = self.embeddings_handles(inputs_handle)\n",
        "        x = torch.cat([embed_tweet, embed_handle], dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HSjayEKbvEoD"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#Hyperparameters\n",
        "num_words = 10000\n",
        "num_handles = 10000\n",
        "embedding_dim = 200\n",
        "hidden_dim = 2*embedding_dim\n",
        "lr = 0.005\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "model = Model(num_words, num_handles, embedding_dim, hidden_dim, num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "#Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    num_batches = len(train_inputs_tweet2) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "\n",
        "        inputs_tweet = torch.LongTensor(train_inputs_tweet2[start_idx:end_idx])\n",
        "        inputs_handle = torch.LongTensor(train_inputs_handle2[start_idx:end_idx])\n",
        "        labels = torch.LongTensor(train_labels2[start_idx:end_idx])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs_tweet, inputs_handle)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_acc += accuracy_score(labels.numpy(), preds.numpy())\n",
        "\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    train_acc /= num_batches\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch+1, num_epochs, train_loss, train_acc))\n"
      ],
      "metadata": {
        "id": "MbWGLU_nC-dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "def test(model, test_inputs_tweet, test_inputs_handle, test_labels, batch_size):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    num_batches = len(test_inputs_tweet) // batch_size\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = (i + 1) * batch_size\n",
        "\n",
        "            inputs_tweet = torch.LongTensor(test_inputs_tweet[start_idx:end_idx])\n",
        "            inputs_handle = torch.LongTensor(test_inputs_handle[start_idx:end_idx])\n",
        "            labels = torch.LongTensor(test_labels[start_idx:end_idx])\n",
        "\n",
        "            outputs = model(inputs_tweet, inputs_handle)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_acc += accuracy_score(labels.numpy(), preds.numpy())\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    test_acc /= num_batches\n",
        "\n",
        "    print('Test Acc: {:.4f}'.format(test_acc))\n"
      ],
      "metadata": {
        "id": "0URVLG1avuYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Description\n",
        "- Loaded the training and test data from CSV files and preprocesses it by removing URLs, mentions, hashtags, and non-alphabetic characters. The labels (political parties) are then encoded using the LabelEncoder from scikit-learn.\n",
        "\n",
        "- Built a vocabulary of the top N most frequent words in the training data and creates a mapping from words to unique integers. The tweets in the training and test data are then converted from text to sequences of integers using the mapping.\n",
        "\n",
        "- The handles are also converted to sequences of integers, padded with zeros to a maximum length of 10, and fed into a separate embedding layer in the model architecture.\n",
        "\n",
        "- Embedded layer for the tweet inputs, an embedding layer for the handle inputs, a linear layer to combine the tweet and handle embeddings, a ReLU activation function, a dropout layer to prevent overfitting, and a linear layer to output the predicted political party.\n",
        "\n",
        "- Finally, the model is trained on the preprocessed training data using the Adam optimizer and cross-entropy loss, and evaluated on the preprocessed test data using accuracy as the evaluation metric.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2CWhCo4vE40T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "my_list = [['Final Model', 'embedding_dim = 200, batch_size = 32, lr = 0.005, num_epochs = 10', '96.7%'], ['Model 1', 'embedding_dim = 100, batch_size = 32, lr = 0.01, num_epochs = 5', '93.1%'], ['Model 2', 'embedding_dim = 150, batch_size = 32, lr= 0.01, num_epochs = 5', '94.5%'],['Model 3', 'embedding_dim = 200, batch_size = 32, lr = 0.005, num_epochs = 10', '96%']]\n",
        "print(tabulate(my_list, headers=['Model', 'Hyperparameters', 'Test Accuracy'], tablefmt='grid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0-cAdfSuCVw",
        "outputId": "85f2ca86-8677-4126-b8bb-3b03b9567054"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------------------------------------------------------------+-----------------+\n",
            "| Model       | Hyperparameters                                                   | Test Accuracy   |\n",
            "+=============+===================================================================+=================+\n",
            "| Final Model | embedding_dim = 200, batch_size = 32, lr = 0.005, num_epochs = 10 | 96.7%           |\n",
            "+-------------+-------------------------------------------------------------------+-----------------+\n",
            "| Model 1     | embedding_dim = 100, batch_size = 32, lr = 0.01, num_epochs = 5   | 93.1%           |\n",
            "+-------------+-------------------------------------------------------------------+-----------------+\n",
            "| Model 2     | embedding_dim = 150, batch_size = 32, lr= 0.01, num_epochs = 5    | 94.5%           |\n",
            "+-------------+-------------------------------------------------------------------+-----------------+\n",
            "| Model 3     | embedding_dim = 200, batch_size = 32, lr = 0.005, num_epochs = 10 | 96%             |\n",
            "+-------------+-------------------------------------------------------------------+-----------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}